# Numba-CUDA Learning Path

> **Tier 1 Foundation**: Learn the SIMT mental model â€” threads, blocks, grids, memory hierarchy.

## ğŸ“š Documentation

- **Official Docs**: https://nvidia.github.io/numba-cuda/
- **Local Source**: `numba-cuda/docs/source/`
- **Examples**: `numba-cuda/numba_cuda/numba/cuda/tests/doc_examples/`

## ğŸ¯ Learning Progression

### Level 0: Setup & Verify
```bash
pip install numba-cuda numpy
python -c "from numba import cuda; print(cuda.gpus)"
```

### Level 1: Thread Indexing (The Mental Model)
**Goal**: Understand `threadIdx`, `blockIdx`, `blockDim`, `gridDim`

| Concept | What it means |
|---------|---------------|
| `cuda.grid(1)` | Your thread's global position |
| `cuda.threadIdx.x` | Position within your block |
| `cuda.blockIdx.x` | Which block you're in |
| `cuda.blockDim.x` | Threads per block |
| `cuda.gridDim.x` | Total number of blocks |

**Exercise**: `exercises/01_thread_identity.py`

### Level 2: Memory Patterns
- **Global Memory**: Slow, all threads can access
- **Shared Memory**: Fast, only threads in same block
- **Local Memory**: Per-thread, for large local arrays

**Exercise**: `exercises/02_shared_memory_reduce.py`

### Level 3: Stencil Computation
**Exercise**: `exercises/03_1d_heat_equation.py`

### Level 4: 2D Grids
**Exercise**: `exercises/04_2d_game_of_life.py`

## ğŸ“ Key Source Files to Study

```
numba-cuda/
â”œâ”€â”€ docs/source/user/
â”‚   â”œâ”€â”€ kernels.rst          # â­ START HERE - kernel basics
â”‚   â”œâ”€â”€ memory.rst           # Memory management
â”‚   â””â”€â”€ examples.rst         # All examples explained
â””â”€â”€ numba_cuda/numba/cuda/tests/doc_examples/
    â”œâ”€â”€ test_vecadd.py       # Vector addition
    â”œâ”€â”€ test_laplace.py      # 1D heat equation
    â”œâ”€â”€ test_reduction.py    # Shared memory reduce
    â””â”€â”€ test_matmul.py       # Matrix multiply (naive + optimized)
```

## ğŸ”— Bridge to cuda.core

After completing numba-cuda exercises, you'll understand:
- Thread/block/grid hierarchy âœ“
- Memory types and access patterns âœ“
- Synchronization (`__syncthreads`) âœ“

Then `cuda.core` will feel natural â€” same concepts, just CUDA C++ kernels instead of Python kernels.

